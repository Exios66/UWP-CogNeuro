---
icon: rectangle-code
coverY: 0
layout:
  cover:
    visible: true
    size: full
  title:
    visible: true
  description:
    visible: false
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
---

# Prompt Injection

### What is Prompt Leaking?

**Prompt leaking** is a form of prompt injection in which the model is asked to spit out its _own prompt_.

{% embed url="https://learnprompting.org/_next/image?q=75&url=/docs/assets/jailbreak/jailbreak_research.webp&w=3840" %}
remoteli.io
{% endembed %}

***

{% embed url="https://learnprompting.org/_next/image?q=75&url=/docs/assets/jailbreak/injection_leak.webp&w=3840" %}

***

As shown in the example image[1](https://learnprompting.org/docs/prompt_hacking/leaking#user-content-fn-1) below, the attacker changes `user_input` to attempt to return the prompt. The intended goal is distinct from goal hijacking (normal prompt injection), where the attacker changes `user_input` to print malicious instructions[1](https://learnprompting.org/docs/prompt_hacking/leaking#user-content-fn-1).

The following image[2](https://learnprompting.org/docs/prompt_hacking/leaking#user-content-fn-2), again from the `remoteli.io` example, shows a Twitter user getting the model to leak its prompt.

{% embed url="https://learnprompting.org/_next/image?q=75&url=/docs/assets/jailbreak/bing_chat.webp&w=3840" %}
`remoteli.io`
{% endembed %}

***

Well, so what? Why should anyone care about prompt leaking?

Sometimes, people want to keep their prompts secret. For example, an education company could use the prompt `explain this to me like I am 5` to explain complex topics. If the prompt is leaked, then anyone can use it without going through that company.

### A Real-World Example of Prompt Leaking: Microsoft Bing Chat

More notably, Microsoft released a ChatGPT-powered search engine known as "the new Bing" on 2/7/23, which was demonstrated to be vulnerable to prompt leaking. The following example by [@kliu128](https://twitter.com/kliu128/status/1623472922374574080) demonstrates how an earlier version of Bing Search, code-named "Sydney", was susceptible when giving a snippet of its prompt[3](https://learnprompting.org/docs/prompt_hacking/leaking#user-content-fn-3). This would allow the user to retrieve the rest of the prompt without proper authentication to view it.

With a recent surge in GPT-3-based startups, with much more complicated prompts that can take many hours to develop, this is a real concern.

### Practice

Try to leak the following prompt[4](https://learnprompting.org/docs/prompt_hacking/leaking#user-content-fn-4) by appending text to it:

**`English`**: I want to go to the park today. **`French`**: Je veux aller au parc aujourd'hui. **`English`**&#x49; like to wear a hat when it rains. **`French`**: J'aime porter un chapeau quand it pleut. **`English`**: What are you doing at school? **`French`**: Qu'est-ce que to fais a l'ecole? **`English`**:

{% embed url="https://codepen.io/Exios66/pen/gbOxbGj" fullWidth="true" %}

> The [Learn Prompting](https://learnprompting.org/) Embed allows you to interactively write and test prompts directly in your website. To start, configure the embed by editing the default parameters:\
>
>
> * [Prompt:](https://learnprompting.org/docs/basics/formalizing) The initial text or question provided to the language model (LLM) as input.
> * [Output:](https://embed.learnprompting.org/) The text generated by the LLM in response to the prompt, based on its training and parameters.
> * [Model:](https://learnprompting.org/docs/basics/world) The underlying structure and trained parameters of the LLM that determine how it processes inputs and generates outputs, with specified context lengths
> * [Temperature:](https://learnprompting.org/docs/basics/configuration_hyperparameters#header-0) A parameter that influences the randomness or creativity of the LLM's responses, with higher values leading to more varied outputs.
> * [Max Tokens:](https://learnprompting.org/docs/basics/configuration_hyperparameters#header-2) The maximum number of tokens (words or subwords) that the LLM is allowed to generate in its response, setting a limit on the length.
> * [Top P:](https://learnprompting.org/docs/basics/configuration_hyperparameters#header-1) A parameter that controls the diversity of the LLM's outputs by limiting token generation to the most probable subset, balancing randomness and coherence.

***

## Conclusion

Prompt leaking is an important concept to understand because the risk of uninentionally exposing sensitive prompts reveals a critical vulnerability in AI-based systems. As more and more businesses rely on language model features, addressing prompt leaking will be crucial to protecting confidential intellectual property.

### FAQ

#### Why is it important to prevent prompt leaking?

Having an **LLM** reveal its original prompt is crucial in case these developer instructions should be kept confidential. Prompt leaking can undermine efforts to create unique prompts and can potentially expose a business's intellectual property.

#### What is a real-world example of prompt leaking?

As demonstrated in the article, prompt leaking can be dangerous to real companies, including Microsoft, whose early Bing chatbot was susceptible to clever inputs that pushed it to unveil its original instructions.

***

### Footnotes <a href="#footnote-label" id="footnote-label"></a>

1. Perez, F., & Ribeiro, I. (2022). Ignore Previous Prompt: Attack Techniques For Language Models. arXiv. [https://doi.org/10.48550/ARXIV.2211.09527](https://doi.org/10.48550/ARXIV.2211.09527) [↩](https://learnprompting.org/docs/prompt_hacking/leaking#user-content-fnref-1) [↩2](https://learnprompting.org/docs/prompt_hacking/leaking#user-content-fnref-1-2)
2. Willison, S. (2022). Prompt injection attacks against GPT-3. [https://simonwillison.net/2022/Sep/12/prompt-injection/](https://simonwillison.net/2022/Sep/12/prompt-injection/) [↩](https://learnprompting.org/docs/prompt_hacking/leaking#user-content-fnref-2)
3. Liu, K. (2023). The entire prompt of Microsoft Bing Chat?! (Hi, Sydney.). [https://twitter.com/kliu128/status/1623472922374574080](https://twitter.com/kliu128/status/1623472922374574080) [↩](https://learnprompting.org/docs/prompt_hacking/leaking#user-content-fnref-3)
4. Chase, H. (2022). adversarial-prompts. [https://github.com/hwchase17/adversarial-prompts](https://github.com/hwchase17/adversarial-prompts) [↩](https://learnprompting.org/docs/prompt_hacking/leaking#user-content-fnref-4)
